/**
 * OpenRouter API Integration
 * Provides a unified interface to multiple LLM providers
 */

import { config } from './config';
import { Message } from '../../types/chat';
import { supabase } from '../../lib/supabase';

// Error types (reused from openai.ts)
export enum ErrorType {
  AUTHENTICATION = 'authentication',
  RATE_LIMIT = 'rate_limit',
  QUOTA_EXCEEDED = 'quota_exceeded',
  INVALID_REQUEST = 'invalid_request',
  SERVER_ERROR = 'server_error',
  UNKNOWN = 'unknown',
}

export interface LLMError {
  type: ErrorType;
  message: string;
  statusCode?: number;
  original?: any;
}

// Interface for streaming callbacks
export interface StreamCallbacks {
  onChunk: (chunk: string) => void;
  onThinkingChunk?: (chunk: string) => void;
  onComplete?: () => void;
  onError?: (error: LLMError) => void;
}

// OpenRouter multimodal content types
interface TextContent {
  type: 'text';
  text: string;
}

interface ImageContent {
  type: 'image_url';
  image_url: {
    url: string;
    detail?: string;
  };
}

type ContentPart = TextContent | ImageContent;

// Supported OpenRouter models
export const SUPPORTED_MODELS = {
  // OpenAI models
  'openai/gpt-4.1': 'GPT-4.1',
  'openai/o4-mini-high': 'OpenAI o4 Mini High',
  
  // Google models
  'google/gemini-2.5-flash-preview': 'Gemini 2.5 Flash Preview',
  'google/gemini-2.5-pro-preview-03-25': 'Gemini 2.5 Pro Preview',
  
  // Anthropic models
  'anthropic/claude-3.7-sonnet': 'Claude 3.7 Sonnet',
  'anthropic/claude-3.7-sonnet:thinking': 'Claude 3.7 Sonnet with Thinking',
  
  // xAI models
  'x-ai/grok-3-mini-beta': 'Grok 3 Mini Beta',
  'x-ai/grok-3-beta': 'Grok 3 Beta'
};

// New function to check if a model is a reasoning model that supports thinking traces
export function isReasoningModel(model: string): boolean {
  // These models support explicit reasoning/thinking traces
  const isReasoning = (
    model.includes(':thinking') ||       // Claude with thinking
    model.startsWith('x-ai/grok') ||     // All Grok models have reasoning
    model === 'openai/o4-mini-high' ||   // o4-mini-high has reasoning capabilities
    model === 'google/gemini-2.5-pro-preview-03-25' // Gemini Pro has reasoning capabilities
  );
  
  console.log(`Model ${model} is ${isReasoning ? '' : 'NOT '}a reasoning model`);
  return isReasoning;
}

// Convert our message format to OpenRouter format
function convertToOpenRouterMessages(messages: Message[]): any[] {
  // Filter out any empty messages, safely handling content which might be a string or an array (for multimodal)
  return messages
    .filter(msg => {
      // If content is a string, check if it's non-empty after trimming
      if (typeof msg.content === 'string') {
        return msg.content && msg.content.trim() !== '';
      } 
      // If we have images in metadata, consider it non-empty even with empty text
      else if (msg.metadata?.images && Array.isArray(msg.metadata.images) && msg.metadata.images.length > 0) {
        return true;
      }
      // Otherwise check if we have any content (this should never happen in our app flow)
      return !!msg.content;
    })
    .map(msg => {
      // Check if this message has images in its metadata
      if (msg.metadata?.images && Array.isArray(msg.metadata.images) && msg.metadata.images.length > 0) {
        // This is a multimodal message - create content parts array
        const contentParts: ContentPart[] = [
          { type: "text", text: msg.content as string }
        ];
        
        // Add image parts
        msg.metadata.images.forEach((imageUrl: string) => {
          contentParts.push({
            type: "image_url",
            image_url: {
              url: imageUrl,
              detail: "auto"
            }
          });
        });
        
        return {
          role: msg.role,
          content: contentParts
        };
      }
      
      // Regular text-only message
      return {
        role: msg.role,
        content: msg.content
      };
    });
}

// Validate that the configured model is supported
function validateModel(model: string): string {
  if (!model || !Object.keys(SUPPORTED_MODELS).includes(model)) {
    console.warn(`Unsupported model: ${model}. Defaulting to anthropic/claude-3.7-sonnet`);
    return 'anthropic/claude-3.7-sonnet';
  }
  return model;
}

// Initialize OpenRouter
export function initializeClient(): void {
  // No need to check for API key here as it's now handled server-side
  
  // Validate the configured model
  const validModel = validateModel(config.openRouterModel);
  if (validModel !== config.openRouterModel) {
    console.warn(`Configured model ${config.openRouterModel} is not supported. Using ${validModel} instead.`);
    // Note: Ideally we would update config.openRouterModel here, but we're assuming it's readonly
  }
  
  console.log('OpenRouter client initialized');
}

// Get available models
export function getAvailableModels(): { id: string, name: string }[] {
  return Object.entries(SUPPORTED_MODELS).map(([id, name]) => ({ id, name }));
}

// Generate completion using OpenRouter API via Supabase Edge Function
export async function generateCompletion(
  messages: Message[]
): Promise<string> {
  try {
    const model = validateModel(config.openRouterModel);
    const systemMessage = { role: 'system' as const, content: config.systemPrompt };
    const openRouterMessages = [systemMessage, ...convertToOpenRouterMessages(messages)];
    
    // Add special params for Grok models
    const requestBody: any = {
      model: model,
      messages: openRouterMessages,
      temperature: config.temperature,
      max_tokens: config.maxTokens,
      stream: false,
    };
    
    // Add reasoning parameter for Grok models
    if (model.startsWith('x-ai/grok-')) {
      requestBody.reasoning = { effort: "high" };
    }

    // Check if we're using a reasoning model that can provide thinking traces
    const isReasoning = isReasoningModel(model);
    
    // Special handling for Claude with thinking
    if (model.includes(":thinking")) {
      console.log("DEBUG - Configuring Claude model with thinking:", model);
      
      // Set high max_tokens for Claude thinking (per OpenRouter docs example)
      requestBody.max_tokens = 16000;
      
      // Use the standardized reasoning parameter with a large gap (per docs example)
      requestBody.reasoning = {
        max_tokens: 8000,  // 8000 tokens for reasoning (must be less than max_tokens)
        effort: "high"
      };
      
      // Remove any other parameters that might conflict
      delete requestBody.claude_settings;
      delete requestBody.anthropic_version;
      delete requestBody.stream_options;
      
      console.log("DEBUG - Claude thinking configuration:", {
        model: model,
        max_tokens: requestBody.max_tokens,
        reasoning: requestBody.reasoning
      });
    }
    
    console.log("DEBUG - Request configuration for reasoning:", {
      model,
      isReasoningModel: isReasoning,
      requestBodyKeys: Object.keys(requestBody),
      hasReasoning: !!requestBody.reasoning,
      systemPromptLength: requestBody.messages?.[0]?.content.length
    });

    // Call our Supabase Edge Function instead of OpenRouter directly
    const { data, error } = await supabase.functions.invoke('llm-api', {
      body: {
        endpoint: 'chat/completions',
        payload: requestBody
      }
    });
    
    if (error) {
      throw handleApiError(500, { error: error.message });
    }
    
    return data.choices[0].message.content;
    
  } catch (error: unknown) {
    console.error('Error generating completion from OpenRouter:', error);
    const llmError = parseError(error);
    throw llmError;
  }
}

// Generate a chat completion from OpenRouter, streaming the response
export async function generateCompletionStream(
  messages: Message[],
  callbacks: StreamCallbacks,
  additionalSystemPrompt: string | null
): Promise<void> {
  try {
    const model = validateModel(config.openRouterModel);
    
    // Combine system prompts
    const combinedSystemPrompt = config.systemPrompt + 
      (additionalSystemPrompt ? `\n\n${additionalSystemPrompt}` : '');
      
    const systemMessage = { role: 'system' as const, content: combinedSystemPrompt };
    const openRouterMessages = [systemMessage, ...convertToOpenRouterMessages(messages)];
    
    // Special validation for Grok models which are sensitive to empty messages
    if (model.startsWith('x-ai/grok-')) {
      // Check if we have at least one non-system message
      const hasNonSystemMessage = openRouterMessages.some(msg => 
        msg.role !== 'system' && 
        // Handle both string content and array content (multimodal)
        ((typeof msg.content === 'string' && msg.content.trim() !== '') || 
         (Array.isArray(msg.content) && msg.content.length > 0))
      );
      
      if (!hasNonSystemMessage) {
        throw new Error('Grok models require at least one non-empty user message');
      }
      
      // Check for any empty messages (shouldn't happen due to our filter, but just in case)
      const emptyMessages = openRouterMessages.filter(msg => 
        (typeof msg.content === 'string' && (!msg.content || msg.content.trim() === '')) ||
        (Array.isArray(msg.content) && msg.content.length === 0)
      );
      if (emptyMessages.length > 0) {
        throw new Error('Grok models do not support empty messages');
      }
    }
    
    // Add special params for Grok models
    const requestBody: any = {
      model: model,
      messages: openRouterMessages,
      temperature: config.temperature,
      max_tokens: config.maxTokens,
      stream: true,
    };
    
    // Add reasoning parameter for Grok models
    if (model.startsWith('x-ai/grok-')) {
      requestBody.reasoning = { effort: "high" };
    }

    // Check if we're using a reasoning model that can provide thinking traces
    const isReasoning = isReasoningModel(model);

    // Special handling for Claude with thinking
    if (model.includes(":thinking")) {
      console.log("DEBUG - Configuring Claude model with thinking:", model);
      
      // Set high max_tokens for Claude thinking (per OpenRouter docs example)
      requestBody.max_tokens = 16000;
      
      // Use the standardized reasoning parameter with a large gap (per docs example)
      requestBody.reasoning = {
        max_tokens: 8000,  // 8000 tokens for reasoning (must be less than max_tokens)
        effort: "high"
      };
      
      // Remove any other parameters that might conflict
      delete requestBody.claude_settings;
      delete requestBody.anthropic_version;
      delete requestBody.stream_options;
      
      console.log("DEBUG - Claude thinking configuration:", {
        model: model,
        max_tokens: requestBody.max_tokens,
        reasoning: requestBody.reasoning
      });
    }
    
    console.log("DEBUG - Request configuration for reasoning:", {
      model,
      isReasoningModel: isReasoning,
      requestBodyKeys: Object.keys(requestBody),
      hasReasoning: !!requestBody.reasoning,
      systemPromptLength: requestBody.messages?.[0]?.content.length
    });

    // Return to using direct fetch but with proper CORS settings
    const response = await fetch(`${import.meta.env.VITE_SUPABASE_URL}/functions/v1/llm-api`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_ANON_KEY}`,
      },
      body: JSON.stringify({
        endpoint: 'chat/completions',
        payload: requestBody
      }),
      // Don't include credentials as this can cause CORS issues
    });
    
    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw handleApiError(response.status, errorData);
    }
    
    if (!response.body) {
      throw new Error('Response body is null');
    }
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value);
      const lines = chunk
        .split('\n')
        .filter(line => line.trim() !== '' && line.trim() !== 'data: [DONE]');
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          try {
            const jsonStr = line.substring(6);
            if (jsonStr === '[DONE]') continue;
            
            const data = JSON.parse(jsonStr);
            
            // Check for error response first with improved error detection
            if (data.error) {
              const errorMessage = data.error.message || 'Unknown provider error';
              const errorCode = data.error.code || 'unknown';
              console.error('Provider returned error:', data);
              
              // Extract further details from metadata if available
              let errorDetails = 'No additional details';
              let rawProviderError = '';
              
              if (data.error.metadata) {
                errorDetails = JSON.stringify(data.error.metadata);
                
                // For Grok models, extract the raw provider error
                if (data.error.metadata.raw) {
                  try {
                    const rawError = JSON.parse(data.error.metadata.raw);
                    if (rawError.error) {
                      rawProviderError = rawError.error;
                    }
                  } catch (e) {
                    // If we can't parse the raw error, use it as is
                    rawProviderError = data.error.metadata.raw;
                  }
                }
              }
              
              console.error(`OpenRouter error: ${errorMessage} (${errorCode}) - ${errorDetails}`, data);
              
              // Create an error with the provider-specific error message if available
              const errorMsg = rawProviderError 
                ? `Provider error: ${rawProviderError}` 
                : `Provider error: ${errorMessage} (${errorCode})`;
              
              const err = new Error(errorMsg);
              (err as any).originalError = data.error;
              (err as any).statusCode = errorCode;
              (err as any).metadata = data.error.metadata;
              throw err;
            }
            
            // Extract thinking trace for reasoning models
            if (isReasoning && data.choices && callbacks.onThinkingChunk) {
              // Debug log: Log raw response structure for reasoning models
              console.log('DEBUG - Full API response for reasoning model:', JSON.stringify(data, null, 2));
              
              console.log('DEBUG - Reasoning model response structure:', JSON.stringify({
                hasChoices: !!data.choices,
                choicesLength: data.choices?.length,
                firstChoiceKeys: data.choices?.[0] ? Object.keys(data.choices[0]) : null,
                deltaKeys: data.choices?.[0]?.delta ? Object.keys(data.choices[0].delta) : null,
                messageKeys: data.choices?.[0]?.message ? Object.keys(data.choices[0].message) : null,
                hasDebug: !!data.debug,
                debugKeys: data.debug ? Object.keys(data.debug) : null,
                responseKeys: Object.keys(data)
              }));
              
              // Check multiple possible locations for thinking content
              const thinkingContent = 
                // Main potential locations
                data.choices[0]?.thinking_trace || // Standard format
                data.choices[0]?.delta?.thinking_trace || // Delta format
                data.choices[0]?.message?.thinking || // Message thinking format
                data.choices[0]?.thinking || // Root level thinking
                data.choices[0]?.reasoning || // Direct reasoning field
                data.choices[0]?.delta?.reasoning || // Delta reasoning field
                data.choices[0]?.message?.reasoning || // Message reasoning field
                
                // Claude-specific locations
                data.choices[0]?.message?.claude_thinking || // Claude thinking
                data.choices[0]?.delta?.claude_thinking || // Claude thinking in delta
                
                // Tool/function call formats
                (data.choices[0]?.delta?.tool_calls?.find((t: any) => t.type === 'thinking')?.thinking?.thoughts) || // Tool calls format
                data.choices[0]?.message?.tool_calls?.find((t: any) => t.type === 'thinking')?.thinking?.thoughts || // Message tool calls
                
                // Debug objects
                (data.debug?.thinking || data.debug?.thinking_trace) || // Debug object (Claude)
                (data.choices[0]?.message?.debug?.thinking || data.choices[0]?.message?.debug?.thinking_trace) || // Message debug
                (data.choices[0]?.message?.hidden?.thinking || data.choices[0]?.message?.hidden?.thinking_trace) || // Hidden metadata (Claude)
                
                // Last resort lookups
                (data.thinking_trace || data.thinking || data.reasoning || ''); // Fallback to root object
              
              // Find where the thinking content was found
              let thinkingContentSource = 'Not found';
              if (data.choices[0]?.thinking_trace) thinkingContentSource = 'choices[0].thinking_trace';
              else if (data.choices[0]?.delta?.thinking_trace) thinkingContentSource = 'choices[0].delta.thinking_trace';
              else if (data.choices[0]?.message?.thinking) thinkingContentSource = 'choices[0].message.thinking';
              else if (data.choices[0]?.thinking) thinkingContentSource = 'choices[0].thinking';
              else if (data.choices[0]?.reasoning) thinkingContentSource = 'choices[0].reasoning';
              else if (data.choices[0]?.delta?.reasoning) thinkingContentSource = 'choices[0].delta.reasoning';
              else if (data.choices[0]?.message?.reasoning) thinkingContentSource = 'choices[0].message.reasoning';
              else if (data.choices[0]?.message?.claude_thinking) thinkingContentSource = 'choices[0].message.claude_thinking';
              else if (data.choices[0]?.delta?.claude_thinking) thinkingContentSource = 'choices[0].delta.claude_thinking';
              // etc. for other paths
              
              console.log('DEBUG - Thinking trace check results:', {
                standardFormat: !!data.choices[0]?.thinking_trace,
                deltaFormat: !!data.choices[0]?.delta?.thinking_trace,
                toolCallsFormat: !!(data.choices[0]?.delta?.tool_calls?.find((t: any) => t.type === 'thinking')?.thinking?.thoughts),
                messageToolCalls: !!(data.choices[0]?.message?.tool_calls?.find((t: any) => t.type === 'thinking')?.thinking?.thoughts),
                directMessageThinking: !!data.choices[0]?.message?.thinking,
                rootLevelThinking: !!data.choices[0]?.thinking,
                reasoningField: !!data.choices[0]?.reasoning || !!data.choices[0]?.delta?.reasoning || !!data.choices[0]?.message?.reasoning,
                claudeThinking: !!data.choices[0]?.message?.claude_thinking || !!data.choices[0]?.delta?.claude_thinking,
                debugObject: !!(data.debug?.thinking || data.debug?.thinking_trace),
                messageDebug: !!(data.choices[0]?.message?.debug?.thinking || data.choices[0]?.message?.debug?.thinking_trace),
                hiddenMetadata: !!(data.choices[0]?.message?.hidden?.thinking || data.choices[0]?.message?.hidden?.thinking_trace),
                rootFallback: !!(data.thinking_trace || data.thinking || data.reasoning),
                thinkingContentSource: thinkingContentSource,
                finalResult: !!thinkingContent,
                contentLength: thinkingContent ? thinkingContent.length : 0
              });
              
              if (thinkingContent) {
                console.log('Thinking trace detected:', thinkingContent.length, 'chars');
                callbacks.onThinkingChunk(thinkingContent);
              } else {
                console.log('No thinking content found in response for reasoning model. Response structure:', 
                  JSON.stringify({
                    hasChoices: !!data.choices,
                    firstChoice: data.choices?.[0] ? Object.keys(data.choices[0]) : null,
                    hasDebug: !!data.debug,
                    responseKeys: Object.keys(data)
                  })
                );
              }
            }
            
            // Handle different response formats for standard content
            let content = null;
            
            // Standard OpenAI/Claude format
            if (data.choices && data.choices[0]?.delta?.content) {
              content = data.choices[0].delta.content;
            } 
            // Grok models sometimes use this format
            else if (data.choices && data.choices[0]?.message?.content) {
              content = data.choices[0].message.content;
            }
            // Grok models can have a different format with thinking trace
            else if (data.choices && data.choices[0]?.thinking_trace) {
              content = data.choices[0].thinking_trace;
            }
            // Grok delta with message content
            else if (data.choices && data.choices[0]?.delta?.message?.content) {
              content = data.choices[0].delta.message.content;
            }
            // Fallback for any other content format we can find
            else if (data.content) {
              content = data.content;
            }
            
            if (content) {
              callbacks.onChunk(content);
            } 
            // No need to log here if no content is found, 
            // as some SSE chunks are expected to be metadata only.
          } catch (e) {
            console.warn('Error parsing SSE chunk:', e);
            // If the chunk contains error data, handle it properly
            if (line.includes('"error"')) {
              try {
                const errorData = JSON.parse(line.substring(6));
                
                if (errorData.error) {
                  const errorMessage = errorData.error.message || 'Unknown provider error';
                  const errorCode = errorData.error.code || 'unknown';
                  
                  // Extract further details from metadata if available
                  let errorDetails = 'No additional details';
                  let rawProviderError = '';
                  
                  if (errorData.error.metadata) {
                    errorDetails = JSON.stringify(errorData.error.metadata);
                    
                    // For Grok models, extract the raw provider error
                    if (errorData.error.metadata.raw) {
                      try {
                        const rawError = JSON.parse(errorData.error.metadata.raw);
                        if (rawError.error) {
                          rawProviderError = rawError.error;
                        }
                      } catch (e) {
                        // If we can't parse the raw error, use it as is
                        rawProviderError = errorData.error.metadata.raw;
                      }
                    }
                  }
                  
                  console.error(`OpenRouter error: ${errorMessage} (${errorCode}) - ${errorDetails}`, errorData);
                  
                  // Create an error with the provider-specific error message if available
                  const errorMsg = rawProviderError 
                    ? `Provider error: ${rawProviderError}` 
                    : `Provider error: ${errorMessage} (${errorCode})`;
                  
                  const err = new Error(errorMsg);
                  (err as any).originalError = errorData.error;
                  (err as any).statusCode = errorCode;
                  (err as any).metadata = errorData.error.metadata;
                  throw err;
                }
              } catch (parseError) {
                console.error('Failed to parse error data:', parseError, 'Original line:', line);
              }
            }
          }
        }
      }
    }
    
    if (callbacks.onComplete) {
      callbacks.onComplete();
    }
  } catch (error: unknown) {
    console.error('Error generating completion stream from OpenRouter:', error);
    let llmError: LLMError;
    
    // Special handling for provider errors with improved details
    if (error instanceof Error) {
      // For provider errors 
      if (error.message.includes('Provider')) {
        // Check for common error patterns
        const isQuotaError = error.message.toLowerCase().includes('quota') || 
          error.message.toLowerCase().includes('rate limit') || 
          error.message.includes('429');
        
        const isModelUnavailable = error.message.toLowerCase().includes('unavailable') ||
          error.message.toLowerCase().includes('not available');
          
        const isEmptyMessageError = error.message.toLowerCase().includes('empty message') ||
          error.message.toLowerCase().includes('empty user message');
        
        if (isQuotaError) {
          llmError = {
            type: ErrorType.QUOTA_EXCEEDED,
            message: 'OpenRouter quota or rate limit exceeded. This is common with free models like Grok and Gemini as they have high demand. Please try again later or use a different model.',
            original: error,
            statusCode: 429
          };
        } else if (isModelUnavailable) {
          llmError = {
            type: ErrorType.SERVER_ERROR,
            message: 'The selected model is currently unavailable on OpenRouter. This sometimes happens with experimental models like Grok. Please try a different model.',
            original: error
          };
        } else if (isEmptyMessageError) {
          // Special handling for Grok's empty message errors
          llmError = {
            type: ErrorType.INVALID_REQUEST,
            message: 'Grok models require at least one non-empty user message. Please ensure your message has content.',
            original: error
          };
        } else {
          // Extract the provider-specific error if available
          let userMessage = error.message;
          // If this is the raw error from the provider, clean it up for display
          if ((error as any).metadata && (error as any).metadata.raw) {
            try {
              const rawError = JSON.parse((error as any).metadata.raw);
              if (rawError.error) {
                userMessage = `OpenRouter error: ${rawError.error}`;
              }
            } catch (e) {
              // If parsing fails, use the original error message
            }
          }
          
          llmError = {
            type: ErrorType.INVALID_REQUEST,
            message: userMessage,
            original: error
          };
        }
      } else {
        llmError = parseError(error);
      }
    } else {
      llmError = parseError(error);
    }
    
    if (callbacks.onError) {
      callbacks.onError(llmError);
    } else {
      throw llmError;
    }
  }
}

// Function to generate a concise conversation title
export async function generateTitle(
  userMessage: string, 
  maxRetries: number = 3,
  defaultTitle: string = "New Chat"
): Promise<string> {
  const maxTitleLength = 30; // Maximum allowed title length
  
  const systemPrompt = `Summarize the following user message into a concise conversation title, maximum ${maxTitleLength} characters. Be brief and capture the main topic. Title:`;

  // Use a powerful model for title generation - prefer OpenAI or Claude
  const titleModel = 'openai/gpt-4.1';
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      // Call our Supabase Edge Function instead of OpenRouter directly
      const { data, error } = await supabase.functions.invoke('llm-api', {
        body: {
          endpoint: 'chat/completions',
          payload: {
            model: titleModel,
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: userMessage }
            ],
            temperature: 0.5,
            max_tokens: 15,
          }
        }
      });
      
      if (error) {
        throw handleApiError(500, { error: error.message });
      }
      
      const generatedTitle = data.choices[0]?.message?.content?.trim();
      
      if (generatedTitle && generatedTitle.length > 0 && generatedTitle.length <= maxTitleLength) {
        return generatedTitle.replace(/^["']|["']$/g, '');
      } else {
        console.warn(`Title generation attempt ${attempt}: Title too long or empty ('${generatedTitle}'). Retrying...`);
      }
    } catch (error: unknown) {
      console.error(`Error generating title on attempt ${attempt}:`, error);
      const llmError = parseError(error);
      if (llmError.type === ErrorType.AUTHENTICATION || llmError.type === ErrorType.QUOTA_EXCEEDED) {
        console.error(`Unrecoverable error encountered (${llmError.type}), stopping title generation.`);
        break;
      }
    }
  }
  
  console.warn(`Failed to generate a valid title after ${maxRetries} attempts.`);
  return defaultTitle;
}

// Helper function to handle API errors
function handleApiError(status: number, data: any): Error {
  let errorMessage = data.error?.message || 'Unknown error occurred';
  let error = new Error(errorMessage);
  
  // Add additional properties
  (error as any).status = status;
  (error as any).data = data;
  
  return error;
}

// Parse and categorize OpenRouter errors
function parseError(error: unknown): LLMError {
  // Default error object
  const defaultError: LLMError = {
    type: ErrorType.UNKNOWN,
    message: 'An unknown error occurred',
    original: error,
  };
  
  // If it's not an Error object, return default
  if (!(error instanceof Error)) {
    return defaultError;
  }
  
  // If it's an API error with status
  if ('status' in error && typeof (error as any).status === 'number') {
    const status = (error as any).status;
    const message = error.message;
    
    // Categorize based on status code
    if (status === 401 || status === 403) {
      return {
        type: ErrorType.AUTHENTICATION,
        message: 'Invalid API key or authentication error',
        statusCode: status,
        original: error,
      };
    } else if (status === 429) {
      // Check if it's rate limit or quota
      if (message.includes('quota') || message.includes('billing')) {
        return {
          type: ErrorType.QUOTA_EXCEEDED,
          message: 'API quota exceeded. Please check your billing information.',
          statusCode: status,
          original: error,
        };
      } else {
        return {
          type: ErrorType.RATE_LIMIT,
          message: 'API rate limit exceeded. Please try again later.',
          statusCode: status,
          original: error,
        };
      }
    } else if (status >= 400 && status < 500) {
      return {
        type: ErrorType.INVALID_REQUEST,
        message: `Invalid request to API: ${message}`,
        statusCode: status,
        original: error,
      };
    } else if (status >= 500) {
      return {
        type: ErrorType.SERVER_ERROR,
        message: 'API server error. Please try again later.',
        statusCode: status,
        original: error,
      };
    }
  }
  
  // For any other errors
  return {
    type: ErrorType.UNKNOWN,
    message: error.message || defaultError.message,
    original: error,
  };
} 